

# Copyright [1999-2015] Wellcome Trust Sanger Institute and the EMBL-European Bioinformatics Institute
# Copyright [2016-2022] EMBL-European Bioinformatics Institute
# 
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
# 
#      http://www.apache.org/licenses/LICENSE-2.0
# 
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

=head1 NAME

CloudConfig

=head1 SYNOPSIS

    use ExonerateSolexaCloudConfig; 

=head1 DESCRIPTION

CloudConfig analysis configuration.

It imports and sets a number of standard global variables into the
calling package. Without arguments all the standard variables are set,
and with a list, only those variables whose names are provided are set.
The module will die if a variable which doesn't appear in its
C<%Config> hash is asked to be set.

The variables can also be references to arrays or hashes.

Edit C<%Config> to add or alter variables.

All the variables are in capitals, so that they resemble environment
variables.

=head1 CONTACT

B<http://lists.ensembl.org/mailman/listinfo/dev>

=cut

package Bio::EnsEMBL::Analysis::Config::ExonerateSolexaCloudConfig;

use strict;
use vars qw(%Config);

%Config = ( 

           # 
           # S3 data configuration - this data will be fetched by Net::Amazon::S3 
           #   

           S3_DATA  => { 
                           ENSEMBL_CORE_API_BUCKET => {  
                                                    aws_access_key_id => '', 
                                                    aws_secret_key_id => '',
                                                    bucket_name => "ensembl-cloud-cvs",
                                                    key_file_names => ["ensembl_58.tar.gz"], 
                                                      },     
                           ENSEMBL_ANALYSIS_API_BUCKET => {  
                                                    aws_access_key_id => '', 
                                                    aws_secret_key_id => '',
                                                    bucket_name => "ensembl-cloud-cvs",
                                                    key_file_names => ["ensembl_analysis_2010_07_05.tar.gz"], 
                                                     },     
                           ENSEMBL_PIPELINE_API_BUCKET => {  
                                                    aws_access_key_id => '', 
                                                    aws_secret_key_id => '',
                                                    bucket_name => "ensembl-cloud-cvs",
                                                    key_file_names => ["ensembl_pipeline_2010_07_05.tar.gz"], 
                                                    },     
                           #
                           # ANOTHER_BUCKET => { 
                           #                     bucket_name => "s3_bucket_name", 
                           #                     key_file_names => ['file_1.zip','file_2.tar.gz'] , 
                           #                   },  
                         }, 
           #
           # Database configuration - If you plan to analyze data for a species which is already in ensembl, you can use 
           # an Ensembl core database as template for your database 
           # 

           SOURCE_REFERENCE_DB_HOST => {  
                                         -create_reference_db => "1", 
                                         -add_pipeline_tables => "1", 
                                         #-template_ref_dbname => "pan_troglodytes_core_58_21m", 
                                         -template_ref_dbname => "saccharomyces_cerevisiae_core_58_1j", 
                                         -template_ref_host =>   "ensembldb.ensembl.org",
                                         -template_ref_user =>   "anonymous",
                                         -template_ref_port =>   5306, 
                                         -tables => ["meta","meta_coord","analysis","seq_region",
                                                     "seq_region_attrib","assembly","assembly_exception","attrib_type",
                                                     "coord_system","external_db","misc_attrib","seq_region_mapping"],
                                      }, 

           MULTIPLE_MYSQL_SERVERS => 0, 


           #
           # specify the user credentials of your default mysql user with write access
           #

           STANDARD_MYSQL_PIPELINE_WRITE_USER => {  
                                                     -create_user => 1 , 
                                                     -user => 'ensadmin',
                                                     -pass => 'helloworld', # now cloud ? 
                                                 },

          # 
          # this section describes, a bit like Databases.pm - where sequence data for the alignment is stored on S3 and how 
          # to access it. The input_ids for this need a different format. 
          # usually its :   chunk.fa OR chunk.fa::OUTPUT_DIR  - but for S3 its chunk.fa::OUTPUT_DB::LANE_1_DATA 
          # 
          # set aws_access_key_id and aws_secret_key_id to undef if you have public data ( ie readable to world )  
          
           S3_SEQUENCE_DATA  => { 
                                  #  BLOOD=chunk1.fa.gz::OUTPUT_DB::1-1000
                                                                                            
                                  BLOOD         => {   
                                                    aws_access_key_id => undef,  
                                                    aws_secret_key_id => undef, 
                                                    bucket_name => "ensembl-cloud-chunks",
                                                    file_regex  => "chunk", 
                                                  },     

           },
           S3_GENOMIC_FASTA_SEQUENCE => {
                                                    aws_access_key_id => '', 
                                                    aws_secret_key_id => '',
                                                    bucket_name => "ensembl-cloud-fasta",
                                                    file_name   => "human_grch37_toplevel.with_nonref.no_duplicate.softmasked_dusted.fa.gz",
                                         },
     
           S3_ANALYSIS_FILES => {  # don't need this as it can be done in the inital step by a runnable. ie could. 
                                 'analysis_logic_name' => {  
                                                           aws_access_key_id => '', 
                                                           aws_secret_key_id => '',
                                                           bucket_name => "ensembl-cloud-analysis",
                                                           key_file_names => "analysis_file" , 
                                                          } 
                                }, 

         ANALYSIS_BASE_BATCH_CONFIG   => {   
                                       # config for 'batches' which are analyses on the same data grouped together
                                       # ie align blood solexa data to genome 
                                       # make rough models 
                                       # align to rough models 
                                       # refine data  

                                       BLOOD  => { 
                                                   STAGE_1_NR_OUTPUT_DBS       => 1 ,       # initial read alignment 
                                                   STAGE_1_UNITED_OUTPUT_DB    => "DB_SOLEXA_STAGE_1_ALIGN_COMBINED" ,       # initial 

                                                   STAGE_2_OUTPUT_DBNAME       => "DB_SOLEXA_STAGE_2" ,   # location of rough gene models  
                                                   STAGE_3_OUTPUT_DBNAME       => "DB_SOLEXA_STAGE_2" ,   # location of 
                                                                                                          #dna_align_feat from intron alignments  
                                                   STAGE_4_OUTPUT_DBNAME       => "DB_SOLEXA_STAGE_3" ,   # refined models  
                                                   S3_CHUNK_LOC                => "BLOOD",
                                                   ROUGH_MODEL_TRANS_LOC       => "/home/ensembl/cloud_transcript_dumps", 
                                                   # location where rough models are dumped to - we try nfs first...
                                                 } , 
                                 },

);



sub import {
    my ($callpack) = caller(0); # Name of the calling package
    my $pack = shift; # Need to move package off @_

    # Get list of variables supplied, or else all
    my @vars = @_ ? @_ : keys(%Config);
    return unless @vars;

    # Predeclare global variables in calling package
    eval "package $callpack; use vars qw("
         . join(' ', map { '$'.$_ } @vars) . ")";
    die $@ if $@;


    foreach (@vars) {
	if (defined $Config{ $_ }) {
            no strict 'refs';
	    # Exporter does a similar job to the following
	    # statement, but for function names, not
	    # scalar variables:
	    *{"${callpack}::$_"} = \$Config{ $_ };
	} else {
	    die "Error: Config: $_ not known\n";
	}
    }
}

1;
